

%%%%%%%%%%%% cd ~/Files/Creations/R/widals/inst/doc ; R64 CMD Sweave widals.Snw

\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{amsmath, amsthm} %%%%%%%%%%%%%%% MUST BE ADDED
\usepackage{supertabular}
\usepackage{wasysym}
\usepackage{setspace}

\usepackage{Sweave}

\usepackage{tabularx}
\newcolumntype{Y}{>{\footnotesize\raggedright\arraybackslash}X}

%\singlespacing
\onehalfspacing
%\doublespacing

%\usepackage{natbib}

%\usepackage{color}
%\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}
%\definecolor{MyDarkRed}{rgb}{0.4,0.0,0.0} 
%\usepackage[colorlinks=true, urlcolor= MyDarkGreen, linkcolor= MyDarkRed ]{hyperref}
\usepackage{hyperref}

\DeclareCaptionLabelSeparator{space}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}



\newcommand{\ve}{\varepsilon}



\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\0}{\mathbf{0}}



\newcommand{\st}{\mathrm{ \:\: s.t. \:\: }}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bm}[1]{\mbox{\boldmath$#1$}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}

\newcommand{\smss}[1]{^{_{#1}}}

\newcommand{\apri}{\smss{\,(-)}}
\newcommand{\apos}{\smss{\,(+)}}

\newcommand{\betaup}{\rotatebox[origin=c]{12}{$\beta$}}

\newcommand{\ttb}{\hspace{-0.01cm}}

\newcommand{\diag}{\mathsf{diag}}
\newcommand{\minz}{\mathsf{min}}
\newcommand{\maxz}{\mathsf{max}}
\newcommand{\zsin}{\mathsf{sin}}
\newcommand{\zcos}{\mathsf{cos}}

\newcommand{\SE}{\mathsf{SE}}
\newcommand{\range}{\mathsf{range}}

\newcommand{\ndxrng}[2]{#1 \,\!\! : \,\!\! #2}

\newenvironment{DZcaption}[2]%
               {\begin{list}{}{\leftmargin#1\rightmargin#2}\item{}}%
               {\end{list}}



%\VignetteIndexEntry{Package kcirt}
%\VignetteDepends{mvtnorm}
%\VignetteDepends{snowfall}
%\VignetteDepends{corpcor}

\begin{document}

\title{Package \texttt{kcirt Tutorial}}
\author{Dave Zes, Jimmy Lewis, Dana Landis\\
    @ Korn/Ferry International}
\maketitle


%\section{Introduction}




<<eval=TRUE, echo=FALSE>>=
options(width=49)
options(prompt=" ")
options(continue="   ")
@


\subsection{Example I}

We commence with an itty-bitty example.


Suppose we have 6 blocks, the first three contain 3 items each, the last three, 4 items each.  We tap three states (aka scales).

\begin{small}
<<eval=FALSE, results=hide>>=
options(stringsAsFactors=FALSE, width=140)

library(kcirt)

constructMap.ls <- list(
c(1,2,3),
c(1,2,3),
c(1,2,3),
c(1,1,2,3),
c(1,2,2,3),
c(1,2,3,3)
)
@
\end{small}


Create true Lambda.  Keying is done by signing. By default these will be the diagonal elements of the Lambda matrix, $\bs{\Lambda}$.

\begin{small}
<<eval=FALSE, results=hide>>=
set.seed(99999)

mxLambda <- c(
c(1,1,-1),
c(1,-1,1),
c(-1,1,1),
c(1,-1,1,-1),
c(1,-1,1,-1),
c(1,-1,1,-1)
)
@
\end{small}



Create true state covariance structure.

\begin{small}
<<eval=FALSE, results=hide>>=
covEta <- diag(1, 3)
@
\end{small}




Define response format.  For only 2 or 3 items in block, kcirt only accepts "R".  The four item blocks are most/least like.

\begin{small}
<<eval=FALSE, results=hide>>=
qTypes <- c("R", "R", "R", "M", "M", "M")
@
\end{small}


Create true utilities.

\begin{small}
<<eval=FALSE, results=hide>>=
mu <- rep(0, length(mxLambda))
@
\end{small}


Now build model skeleton and examine contents.

\begin{small}
<<eval=FALSE, results=hide>>=
mod1 <- kcirt.model(constructMap.ls=constructMap.ls, qTypes=qTypes, mxLambda=mxLambda, 
mu=mu, covEta=covEta)

mod1
@
\end{small}


Take a peek at the information $\bs{\eta}$ inherits from $\mb{y}^{\star}$.

\begin{small}
<<eval=FALSE, results=hide>>=
kcirt.ystarinfo(mod1)
@
\end{small}


Notice, above, that inasmuch as the loadings have equal magnitude, their keying and the respective location of items within provide equal information to each state.


Let's now draw a random realization from under this parameterized model, and look at the rank data.


\begin{small}
<<eval=FALSE, results=hide>>=
N <- 200

set.seed(99999)
mod2 <- kcirt.sim(mod1, N=N)

mod2$mxData
@
\end{small}


Let's now fit model.  We simulate an initial guess for the loadings, $\bs{\Lambda}$, by adding some noise to the true values.


\begin{small}
<<eval=FALSE, results=hide>>=
mxHatLambda <- matrix(0, nrow(mod2$mxLambda), ncol(mod2$mxLambda))
diag(mxHatLambda) <- diag(mod2$mxLambda) + rnorm(nrow(mod2$mxLambda), 0, 0.2)
@
\end{small}



Set an initial guess for $\bs{\mu}$ at zero.  Set predicted states to zero as starting point.

\begin{small}
<<eval=FALSE, results=hide>>=
hatMu <- rep(0, length(mxLambda))
mxHatEta <- matrix(0, N, 3)
@
\end{small}

Insert these three objects into model.

\begin{small}
<<eval=FALSE, results=hide>>=
mod2$mxHatLambda <- mxHatLambda
mod2$mxHatEta <- mxHatEta
mod2$hatMu <- hatMu
@
\end{small}


\begin{small}
<<eval=FALSE, results=hide>>=
mod3 <- mod2
@
\end{small}



Use stochastic search to locate (internally in order) $\bs{\mu}$, $\bs{\Lambda}$, $\mb{E}$.  View performance (reliabilities), plot $\widehat{\bs{\eta}}$ versus ${\bs{\eta}}$ and $\widehat{\bs{\Lambda}}$ versus $\bs{\Lambda}$.

\begin{small}
<<eval=FALSE, results=hide>>=
mod3 <- kcirt.fitMSS(model=mod3, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0.03, 0.03, 0.5))

mod3$performance
mean(mod3$performance)

par(mfrow=c(1, 4))
for(i in 1:ncol(mod3$mxHatEta)) {
plot(mod3$mxEta[ ,i], mod3$mxHatEta[ ,i],
main = round(1000*cor(mod3$mxHatEta[ ,i], mod3$mxEta[ ,i])^2)/1000 )
}
plot(mod3$mxLambda, mod3$mxHatLambda)

@
\end{small}



Notice the argument \texttt{mss.sd} in \texttt{kcirt.fitMSS()}.  We have supplied it with a three-element vector.  The values are the respective search function standard deviations used when locating $\bs{\mu}$, $\bs{\Lambda}$, $\mb{E}$.  In the example, we have made the search for $\bs{\mu}$ comparitively \emph{tight}, implying a belief that our starting $\widehat{\bs{\mu}}$ is close to ${\bs{\mu}}$.  In the bigger picture --- the actual fitting of our model to data --- these values have little impact as numerous MSS passes can be made over the data, allowing the hatted values to wander about to the fancy of the researcher.




That said, try running the above code block 3 or 4 more times, and notice the average reliability climbs to about 74\%.







\subsection{Example II}

Here we will introduce the notion of item \emph{crosstalk}, and how it might be accounted for in a \texttt{kcirt} model.

In conversational terms, suppose that the preference of items in a block is not only determined by independent comparison, but also by mutual moderation between items within the block.  For example, in a block containing four items, A, B, C, D, a respondent chooses A over B based on the interplay of items B, C, D upon the perceived appropriateness of item A, and the interplay of items A, C, D, upon the perceived appropriateness of item B.

We use the term \emph{moderation} somewhat loosely.  While $\bs{\Lambda} \, \mb{S} \, \bs{\eta}$ is linear, the elements of this matrix product vector are multiplicative with respect to loadings and their respective states.


Let's start with a super itty bitty example.


\begin{small}
<<eval=FALSE, results=hide>>=
constructMap.ls <- list(
c(1,1,2),
c(2,2,3),
c(3,3,1)
)

qTypes <- c("R", "R", "R")

mod1 <- kcirt.model(constructMap.ls=constructMap.ls, qTypes=qTypes)
@
\end{small}



The first row of

\begin{small}
<<eval=FALSE, results=hide>>=
mod1$mxLambdaCTinfo
@
\end{small}

shows the model relationship between the first item and all items on the instrument.

\begin{small}
\begin{verbatim}
"S"  "WT" "WF" "BF" "BF" "BF" "BF" "BF" "BT"
\end{verbatim}
\end{small}

The first element gives the relationship between item 1 and item 1 --- ``S'' for self.  The second element gives the relationship between item 1 and item 2 --- ``WT''. The W stands for ``within'', meaning that item 2 lives in the same question block as item 1.  The ``T'' stands for ``True'', meaning that the state that item 2 points to is the same as the one item 1 points to.  Notice, by looking at \texttt{constructMap.ls}, both these items point to the 1st state (the state called ``1'' in \texttt{constructMap.ls}).  In the fourth element, the ``B'' stands for ``between'', meaning that item 1 and item 4 live in different question blocks, the ``F'' means that they point to different states.








\subsection{Example III}

Here we will deliberately mis-ID a system.



\begin{small}
<<eval=FALSE, results=hide>>=
constructMap.ls <- list(
c(1,2,3),
c(1,2,3),
c(1,2,3),
c(1,1,2,3),
c(1,2,2,3),
c(1,2,3,3)
)


mxLambda <- c(
c(1,1,-1),
c(1,-1,1),
c(-1,1,1),
c(1,-1,1,-1),
c(1,-1,1,-1),
c(1,-1,1,-1)
)


covEta <- diag(1, 3)


qTypes <- c("R", "R", "R", "M", "M", "M")


mu <- rep(0, length(mxLambda))


mod1 <- kcirt.model(constructMap.ls=constructMap.ls, qTypes=qTypes, mxLambda=mxLambda,
mu=mu, covEta=covEta)

@
\end{small}


Add within crosstalk.

\begin{small}
<<eval=FALSE, results=hide>>=
set.seed(99999)
mod1$mxLambda[ mod1$mxLambdaCTinfo == "WF" ] <-
rnorm( sum(mod1$mxLambdaCTinfo == "WF" ), 0, 0.1 )
@
\end{small}


Two-fold validation.

\begin{small}
<<eval=FALSE, results=hide>>=
set.seed(99999)
N <- 1000
modX1 <- kcirt.sim(mod1, N=N)
modX2 <- kcirt.sim(mod1, N=N)
@
\end{small}


Fit the first data set, but incorrectly assume no crosstalk.

\begin{small}
<<eval=FALSE, results=hide>>=

mxHatLambda <- matrix(0, nrow(modX1$mxLambda), ncol(modX1$mxLambda))
diag(mxHatLambda) <- diag(modX1$mxLambda) + rnorm(nrow(modX1$mxLambda), 0, 0.2)

mxHatEta <- matrix(0, N, 3)
hatMu <- rep(0, length(mxLambda))

modX1$mxHatLambda <- mxHatLambda
modX1$mxHatEta <- mxHatEta
modX1$hatMu <- hatMu


mod3 <- modX1


mod3 <- kcirt.fitMSS(model=mod3, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0.03, 0.03, 0.5)) ### first MSS pass
mean(mod3$performance)

mod3 <- kcirt.fitMSS(model=mod3, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0.03, 0.03, 0.5)) ### second MSS pass
mean(mod3$performance)

mod3 <- kcirt.fitMSS(model=mod3, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0.03, 0.03, 0.5)) ### third MSS pass
mean(mod3$performance)



mod3$performance


par(mfrow=c(1, 4))
for(i in 1:ncol(mod3$mxHatEta)) {
plot(mod3$mxEta[ ,i], mod3$mxHatEta[ ,i],
main = round(1000*cor(mod3$mxHatEta[ ,i], mod3$mxEta[ ,i])^2)/1000 )
}
plot(mod3$mxLambda, mod3$mxHatLambda)

@
\end{small}





Shove $\wh{\bs{\mu}}$ and $\wh{\bs{\Lambda}}$ fitted from first half of data into second half of data.  Prevent \texttt{kcirt.fitMSS()} from fitting (altering) $\wh{\bs{\mu}}$ and $\wh{\bs{\Lambda}}$ by turning the first and second elements of \texttt{mss.sd} to zero.



\begin{small}
<<eval=FALSE, results=hide>>=


modX2$hatMu <- mod3$hatMu
modX2$mxHatLambda <- mod3$mxHatLambda
modX2$mxHatEta <- matrix(0, N, 3)

mod4 <- modX2


mod4 <- kcirt.fitMSS(model=mod4, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0, 0, 0.5)) ### first MSS pass
mean(mod4$performance)

mod4 <- kcirt.fitMSS(model=mod4, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0, 0, 0.5)) ### second MSS pass
mean(mod4$performance)

mod4 <- kcirt.fitMSS(model=mod4, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0, 0, 0.5)) ### third MSS pass
mean(mod4$performance)



mod4$performance


par(mfrow=c(1, 4))
for(i in 1:ncol(mod4$mxHatEta)) {
plot(mod4$mxEta[ ,i], mod4$mxHatEta[ ,i],
main = round(1000*cor(mod4$mxHatEta[ ,i], mod4$mxEta[ ,i])^2)/1000 )
}
plot(mod4$mxLambda, mod4$mxHatLambda)

@
\end{small}










\subsection{Example IV}

Lastly we will deliberately mis-ID a system by incorrectly assuming within block crosstalk.



\begin{small}
<<eval=FALSE, results=hide>>=
constructMap.ls <- list(
c(1,2,3),
c(1,2,3),
c(1,2,3),
c(1,1,2,3),
c(1,2,2,3),
c(1,2,3,3)
)


mxLambda <- c(
c(1,1,-1),
c(1,-1,1),
c(-1,1,1),
c(1,-1,1,-1),
c(1,-1,1,-1),
c(1,-1,1,-1)
)


covEta <- diag(1, 3)


qTypes <- c("R", "R", "R", "M", "M", "M")


mu <- rep(0, length(mxLambda))


mod1 <- kcirt.model(constructMap.ls=constructMap.ls, qTypes=qTypes, mxLambda=mxLambda,
mu=mu, covEta=covEta)

@
\end{small}




Again, use two-fold validation.

\begin{small}
<<eval=FALSE, results=hide>>=
set.seed(99999)
N <- 1000
modX1 <- kcirt.sim(mod1, N=N)
modX2 <- kcirt.sim(mod1, N=N)
@
\end{small}


Fit the first data set, but incorrectly assume within block crosstalk.

\begin{small}
<<eval=FALSE, results=hide>>=

mxHatLambda <- matrix(0, nrow(modX1$mxLambda), ncol(modX1$mxLambda))
diag(mxHatLambda) <- diag(modX1$mxLambda) + rnorm(nrow(modX1$mxLambda), 0, 0.2)

mxHatEta <- matrix(0, N, 3)
hatMu <- rep(0, length(mxLambda))

modX1$mxHatLambda <- mxHatLambda
modX1$mxHatEta <- mxHatEta
modX1$hatMu <- hatMu


mod3 <- modX1


mod3 <- kcirt.fitMSS(model=mod3, lambdaConstraint="withinx", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0.03, 0.03, 0.5)) ### first MSS pass
mean(mod3$performance)

mod3 <- kcirt.fitMSS(model=mod3, lambdaConstraint="withinx", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0.03, 0.03, 0.5)) ### second MSS pass
mean(mod3$performance)

mod3 <- kcirt.fitMSS(model=mod3, lambdaConstraint="withinx", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0.03, 0.03, 0.5)) ### third MSS pass
mean(mod3$performance)



mod3$performance


par(mfrow=c(1, 4))
for(i in 1:ncol(mod3$mxHatEta)) {
    plot(mod3$mxEta[ ,i], mod3$mxHatEta[ ,i],
    main = round(1000*cor(mod3$mxHatEta[ ,i], mod3$mxEta[ ,i])^2)/1000 )
}
plot(mod3$mxLambda, mod3$mxHatLambda)

@
\end{small}





Again, shove $\wh{\bs{\mu}}$ and $\wh{\bs{\Lambda}}$ fitted from first half of data into second half of data.  Prevent \texttt{kcirt.fitMSS()} from fitting (altering) $\wh{\bs{\mu}}$ and $\wh{\bs{\Lambda}}$ by turning the first and second elements of \texttt{mss.sd} to zero.

By the way, note that in the following code the argument \texttt{lambdaConstraint} makes no difference since we are in effect not actually fitting ${\bs{\Lambda}}$.


\begin{small}
<<eval=FALSE, results=hide>>=


modX2$hatMu <- mod3$hatMu
modX2$mxHatLambda <- mod3$mxHatLambda
modX2$mxHatEta <- matrix(0, N, 3)

mod4 <- modX2


mod4 <- kcirt.fitMSS(model=mod4, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0, 0, 0.5)) ### first MSS pass
mean(mod4$performance)

mod4 <- kcirt.fitMSS(model=mod4, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0, 0, 0.5)) ### second MSS pass
mean(mod4$performance)

mod4 <- kcirt.fitMSS(model=mod4, lambdaConstraint="self", kcpus=2,
penalty="logit", usetruesigma=TRUE, mss.sd=c(0, 0, 0.5)) ### third MSS pass
mean(mod4$performance)



mod4$performance


par(mfrow=c(1, 4))
for(i in 1:ncol(mod4$mxHatEta)) {
    plot(mod4$mxEta[ ,i], mod4$mxHatEta[ ,i],
    main = round(1000*cor(mod4$mxHatEta[ ,i], mod4$mxEta[ ,i])^2)/1000 )
}
plot(mod4$mxLambda, mod4$mxHatLambda)

@
\end{small}



There seems to be some suggestion of resilience against mis-identification.  Of course, a scientific argument would require many more replications, but this is just a tutorial.

















%\section{Final Thoughts}

%\cite{Wasserman}

%\cite{Efrom}


%\bibliographystyle{plainnat}

%\bibliographystyle{jes}

%\bibliographystyle{abbrv}


%\bibliography{kcirtTutorial}




 \end{document}



<<eval=FALSE, results=hide>>=


setwd("/Users/dzes/KF_Files/KF_Creations/KF_R/kcirt/inst/doc")
Sweave("kcirt.Snw")


@


























